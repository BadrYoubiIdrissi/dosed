{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dosed Training and Evaluation\n",
    "\n",
    "Requirements:\n",
    "\n",
    "1. You need the data for training as h5 files, to get them you either:\n",
    "  - Go through `download_and_data_format_explanation.ipynb`\n",
    "\n",
    "    or \n",
    "\n",
    "  - Run `make download_example`  you can use optionnal DOWNLOAD_PATH env variable to specify an alternative directory\n",
    "  \n",
    "2. Have dosed installed:\n",
    "    \n",
    "  - Run `pip install -e .` from dosed root directory\n",
    "  or \n",
    "  - Run `python setup.py develop` from dosed root directory\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "h5_directory = '../data/h5'  # adapt if you used a different DOWNLOAD_PATH when running `make download_example`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train, validation and test dataset creation\n",
    "\n",
    "## First we select which records we want to train, validate and test on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tempfile\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "from dosed.utils import Compose\n",
    "from dosed.datasets import BalancedEventDataset as dataset\n",
    "from dosed.models import DOSED4 as model\n",
    "from dosed.datasets import get_train_validation_test\n",
    "from dosed.trainers import trainers\n",
    "from dosed.preprocessing import GaussianNoise, RescaleNormal, Invert\n",
    "\n",
    "seed = 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records train: 11\n",
      "Number of records validation: 5\n",
      "Number of records test: 5\n"
     ]
    }
   ],
   "source": [
    "train, validation, test = get_train_validation_test(h5_directory,\n",
    "                                                    percent_test=25,\n",
    "                                                    percent_validation=33,\n",
    "                                                    seed=seed)\n",
    "\n",
    "print(\"Number of records train:\", len(train))\n",
    "print(\"Number of records validation:\", len(validation))\n",
    "print(\"Number of records test:\", len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then we use the dataset class that will be use to generate sample for training and evaluation\n",
    "\n",
    "- h5_directory: Location of the generic h5 files.\n",
    "- signals: the signals from the h5 we want to include together with their normalization\n",
    "- events: the evvents from the h5 we want to train on\n",
    "- window: Spindles have a duration of ~1 seconds, so we design the samples accordingly by choosing 10 seconds windows\n",
    "- ratio_positive: sample within a training batch will have a probability of \"ratio_positive\" to contain at least one spindle \n",
    "- n_jobs: number of process used to extract and normalize signals from h5 files.\n",
    "- cache_data: cache results of extraction and normalization of signals from h5_file in h5_directory + \"/.cache\" (we strongly recommand to set True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 10  # window duration in seconds\n",
    "ratio_positive = 0.5  # When creating the batch, sample containing at least one spindle will be drawn with that probability\n",
    "\n",
    "signals = [\n",
    "    {\"name\": \"eeg_raw\",\n",
    "     \"fs\": 32,\n",
    "     \"signals\": [{\"h5_paths\": [\"/eeg_0\", \"/eeg_1\"],\n",
    "                  \"fs\": 64}],\n",
    "     \"preprocessing\": [\n",
    "         {\"name\": \"clip_and_normalize\",\n",
    "          \"args\": {\n",
    "              \"min_value\": -150,\n",
    "              \"max_value\": 150,\n",
    "          }}\n",
    "     ],\n",
    "     },\n",
    "\n",
    "    {\"name\": \"eeg_spectrogram\",\n",
    "     \"fs\": 64,\n",
    "     \"signals\": [{\"h5_paths\": [\"/eeg_0\", \"/eeg_1\"],\n",
    "                  \"fs\": 64}],\n",
    "     \"preprocessing\": [\n",
    "         {\"name\": \"spectrogram\",\n",
    "          \"args\": {\n",
    "                  \"nperseg\": 8,\n",
    "                  \"nfft\": 16,\n",
    "                  \"temporal_downsampling\": 1,\n",
    "                  \"frequential_downsampling\": 1,\n",
    "                  \"padded\": True,\n",
    "          }},\n",
    "         {\"name\": \"clip_and_normalize\",\n",
    "          \"args\": {\n",
    "                  \"min_value\": 0,\n",
    "                  \"max_value\": 200,\n",
    "          }},\n",
    "     ],\n",
    "     }\n",
    "]\n",
    "\n",
    "events = [\n",
    "    {\n",
    "        \"name\": \"spindle\",\n",
    "        \"h5_path\": \"spindle\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 680.25it/s]\n",
      "100%|██████████| 5/5 [00:00<00:00, 1445.91it/s]\n",
      "100%|██████████| 11/11 [00:00<00:00, 3532.45it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_parameters = {\n",
    "    \"h5_directory\": h5_directory,\n",
    "    \"signals\": signals,\n",
    "    \"events\": events,\n",
    "    \"window\": window,\n",
    "    \"ratio_positive\": ratio_positive,\n",
    "    \"n_jobs\": -1,  # Make use of parallel computing to extract and normalize signals from h5\n",
    "    \"cache_data\": True,  # by default will store normalized signals extracted from h5 in h5_directory + \"/.cache\" directory\n",
    "}\n",
    "\n",
    "dataset_validation = dataset(records=validation, **dataset_parameters)\n",
    "dataset_test = dataset(records=test, **dataset_parameters)\n",
    "\n",
    "# for training add data augmentation\n",
    "dataset_parameters_train = {\n",
    "    \"transformations\": Compose([\n",
    "        GaussianNoise(),\n",
    "        RescaleNormal(),\n",
    "        Invert(),\n",
    "    ])\n",
    "}\n",
    "dataset_parameters_train.update(dataset_parameters)\n",
    "dataset_train = dataset(records=train, **dataset_parameters_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create a network\n",
    "\n",
    "The main parameters for the network are:\n",
    "  - default event sizes : to choose according to a priori size of the event to detect, here spindles are around 1 second\n",
    "  - conv_spec_layers : layers for data as spectrogram\n",
    "  - conv_raw_layers : layers for raw data\n",
    "\n",
    "      Both are a list of layers, where each layer is a tuple :\n",
    "      (channels, kernel, stride, padding, pooling)\n",
    "      - channels : the number of output channels\n",
    "      - kernel : the kernel to be convoluted (2D for spectrogram, 1D for raw signal)\n",
    "      - stride : the stride of each convolution (2D for spectrogram, 1D for raw signal)\n",
    "      - padding : the padding before the convolution (2D for spectrogram, 1D for raw signal)\n",
    "      - pooling : the pooling's size (2D for spectrogram, 1D for raw signal)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_event_sizes = [0.7, 1, 1.3]\n",
    "probability_dropout = 0.1\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "convolution_layers = {\n",
    "    \"eeg_spectrogram\":\n",
    "        \"[[4 , (1,3), (1,2), (0,2), (2,2)]] + \" + \\\n",
    "        \"[[8 , (1,3), (1,2), (0,2), (2,2)]] + \" + \\\n",
    "        \"[[16, (1,3), (1,1), (0,2), (2,2)]] + \" + \\\n",
    "        \"[[32, (1,3), (1,1), (0,2), (1,2)]] + \" + \\\n",
    "        \"[[64, (1,3), (1,1), (0,2), (1,2)]]\",\n",
    "        #(channels, kernel, stride, padding, pooling)\n",
    "    \"eeg_raw\":\n",
    "        \"[[4 , (5,), (1,), (2,), (2,)]] + \" + \\\n",
    "        \"[[8 , (5,), (1,), (2,), (2,)]] + \" + \\\n",
    "        \"[[16 , (5,), (1,), (2,), (2,)]] + \" + \\\n",
    "        \"[[32, (5,), (1,), (2,), (2,)]] + \" + \\\n",
    "        \"[[64, (5,), (1,), (2,), (2,)]]\"\n",
    "        #(channels, kernel, stride, padding, pooling)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_frequency = dataset_train.fs\n",
    "\n",
    "net_parameters = {\n",
    "    \"detection_parameters\": {\n",
    "        \"overlap_non_maximum_suppression\": 0.5,\n",
    "        \"classification_threshold\": 0.7\n",
    "    },\n",
    "    \"default_event_sizes\": default_event_sizes,\n",
    "    \"convolution_layers\": convolution_layers,\n",
    "    \"pdrop\": probability_dropout,\n",
    "    \"fs\": sampling_frequency,   # just used to print architecture info with right time\n",
    "    \"input_shapes\": dataset_train.input_shapes,\n",
    "    \"window\": window,\n",
    "    \"number_of_classes\": dataset_train.number_of_classes,\n",
    "}\n",
    "net = model(**net_parameters)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train the network\n",
    "\n",
    "Parameters are\n",
    "  - learning_rate\n",
    "  - loss type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_parameters = {\n",
    "    \"lr\": 5e-3,\n",
    "    \"weight_decay\": 1e-8,\n",
    "}\n",
    "loss_specs = {\n",
    "    \"type\": \"focal\",\n",
    "    \"parameters\": {\n",
    "        \"number_of_classes\": dataset_train.number_of_classes,\n",
    "        \"device\": device,\n",
    "    }\n",
    "}\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda:0\n",
      "\u001b[32;1mLogging data to /tmp/tmp7kjltqmv/train_history.json\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [05:44<56:57, 189.88s/it, best_metric_score=0.0405, last_update=0, threshold=0.605]"
     ]
    }
   ],
   "source": [
    "trainer = trainers[\"adam\"](\n",
    "    net,\n",
    "    optimizer_parameters=optimizer_parameters,\n",
    "    loss_specs=loss_specs,\n",
    "    epochs=epochs,\n",
    ")\n",
    "\n",
    "best_net_train, best_metrics_train, best_threshold_train = trainer.train(\n",
    "    dataset_train,\n",
    "    dataset_validation,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = best_net_train.predict_dataset(\n",
    "    dataset_test,\n",
    "    best_threshold_train,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "record = dataset_test.records[1]\n",
    "\n",
    "index_spindle = 30\n",
    "window_duration = 5\n",
    "\n",
    "# retrive spindle at the right index\n",
    "spindle_start = float(predictions[record][0][index_spindle][0]) / sampling_frequency\n",
    "spindle_end = float(predictions[record][0][index_spindle][1]) / sampling_frequency\n",
    "\n",
    "# center data window on annotated spindle \n",
    "start_window = spindle_start + (spindle_end - spindle_start) / 2 - window_duration\n",
    "stop_window = spindle_start + (spindle_end - spindle_start) / 2 + window_duration\n",
    "\n",
    "# Retrieve EEG data at right index\n",
    "index_start = int(start_window * sampling_frequency)\n",
    "index_stop = int(stop_window * sampling_frequency)\n",
    "y = dataset_test.signals[record][\"data\"][\"raw\"][0][index_start:index_stop]\n",
    "\n",
    "# Build corresponding time support\n",
    "t = start_window + np.cumsum(np.ones(index_stop - index_start) * 1 / sampling_frequency)\n",
    "\n",
    "plt.figure(figsize=(16, 5))\n",
    "plt.plot(t, y)\n",
    "plt.axvline(spindle_end)\n",
    "plt.axvline(spindle_start)\n",
    "plt.ylim([-1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
